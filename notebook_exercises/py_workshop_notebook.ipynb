{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Python Tools for Metadata Assessment: 55-minute workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome\n",
    "\n",
    "Welcome to the workshop! This is meant to be a fun and beginner-friendly introduction to a few useful Python tools, in the context of working with digital collections metadata.\n",
    "\n",
    "In this session, we will focus on some basic functions of Python's pandas data analysis library, which is especially useful for tabular metadata. We will use pandas for exploring, filtering, reshaping, and merging datasets. \n",
    "\n",
    "This notebook provides:\n",
    "* code that you can execute to see results and generate outputs, directly in the notebook\n",
    "* explanations of the code examples\n",
    "* exercises that we will work through together\n",
    "\n",
    "The workshop will be very quick! We will not be able to get through everything. So, this notebook also includes: \n",
    "* a bonus example about working with dates\n",
    "* a list of recommended resources for further exploration on your own. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Workshop Plan](#wplan)\n",
    "* [Using the Jupyter Notebook](#usingjn)\n",
    "    * [Exercise 1: Run Code Cells](#ex1)\n",
    "    * [Exercise 2: Add New Code](#ex2)\n",
    "    * [More Jupyter Notebook Exercises](#mjn)\n",
    "* [Python/pandas Examples with Collections Metadata](#ppe)\n",
    "    * [Metadata Example 1: Explore a dataset](#md1)\n",
    "        * [Exercise 3: Troubleshoot a Datatype Issue](#ex3)\n",
    "    * [Metadata Example 2: Compare a group of metadata files](#md2)\n",
    "        * [Exercise 4: Evaluate Values in a Subjects field](#ex4)\n",
    "    * [Metadata Example 3: Merge information from separate files](#md3)\n",
    "        * [Exercise 5: Other ways to merge](#ex5)\n",
    "    * [Bonus Example: Working with Dates](#md4)\n",
    "* [Resources](#res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Plan <a name=\"wplan\"></a>\n",
    "\n",
    "We will start with a quick demo in using the jupyter notebook for running code cells and adding code to cells.\n",
    "\n",
    "Then we will walk through three examples of using Python/pandas with digital collections metadata files.  \n",
    "\n",
    "After each example there will be an exercise so you can try out working with the collections data.     \n",
    "\n",
    "\n",
    "----\n",
    "  \n",
    "**Intro, & using the jupyter notebook (10 mins)**\n",
    "    * Exercises 1 & 2: how to run code cells, how to add new code into cells \n",
    "    \n",
    "**Example 1: Explore a dataset (12 mins)** \n",
    "    * Exercise 3: Troubleshoot a Datatype Issue\n",
    "    \n",
    "**Example 2: Compare a group of metadata files (12 mins)**\n",
    "    * Exercise 4: Evaluate Values in a Subject field\n",
    "    \n",
    "**Example 3: Merge info from separate files (12 mins)**\n",
    "    * Exercise 5 : Other Ways to Merge\n",
    "    \n",
    "**Wrap-up (5 mins)**\n",
    "    * Review resources, and suggestions about installing python\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Jupyter Notebook <a name=\"usingjn\"></a>\n",
    "\n",
    "## What is a Jupyter Notebook? What is Binder?\n",
    "\n",
    "[Jupyter Notebook](https://jupyter-notebook.readthedocs.io/en/stable/) is a web application for creating and sharing interactive documents -- they combine live code with rich explanatory text. \n",
    "Jupyter notebooks can include helpful formatting such as plots, mathematical equations, and other rich media.  \n",
    "\n",
    "[The Binder Project](https://mybinder.readthedocs.io/en/latest/introduction.html) provides software and computing environments for online sharing of interactive projects, such as Jupyter Notebooks. \n",
    "\n",
    "## How to Use this Notebook \n",
    "You can edit this notebook directly:\n",
    "* run code cells and generate output \n",
    "* add new code and explanatory text \n",
    "\n",
    "In this section we'll go through basic jupyter notebook commands to get familiar with working in the notebook.  \n",
    "\n",
    "All paths in the notebook refer to locations within this project folder, so you can access example data and/or save output files within this project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Run code cells <a name=\"ex1\"></a>\n",
    "\n",
    "Practice running Python code in the notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Code Cells and Markdown Cells\n",
    "\n",
    "All content in this notebook is located within individual cells, which you can identify by clicking on them. Clicking on a cell causes its borders to appear. \n",
    "\n",
    "Jupyter notebooks have two kinds of cells:   \n",
    "\n",
    "1. **Code cells:** contain code that can be executed within the notebook. Code cells have an `In [ ]:` to the left of the cell. \n",
    "\n",
    "2. **Markdown cells:** contain text providing explanation and context, usually about the code cells.\n",
    "    * Markdown is a simple markup language for writing for the web. \n",
    "    * More information about Markdown at [Markdown Guide.org](https://www.markdownguide.org/getting-started/) \n",
    "    \n",
    "Click on the Code and Markdown cells below to see the cell borders and identify what kind of cells they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. example code cell :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hashtags in a code cell indicate a comment -- informational text for a human to read, not to be executed as code\n",
    "\n",
    "print(\"here's an example code cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. example markdown cell:  \n",
    "\n",
    "this cell only has markdown-formatted text. No code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Run a Code Cell\n",
    "\n",
    "To identify a code cell, click on the cell. You'll see the cell border appear. \n",
    "\n",
    "To run the code in the cell, press `CTRL` + `Enter`.\n",
    "\n",
    "While the code runs, you'll see an `*` in the `In [ ]:` to the left of the cell. \n",
    "\n",
    "When the code completes, an integer will appear in the `In [ ]:` to the left of the cell.   \n",
    "\n",
    "If the code produced output, the output will appear underneath the code cell.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try running this code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code cell demonstrating print output \n",
    "\n",
    "print(\"Press CTRL + ENTER to run a code cell. If the code produces output, it will appear below the cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More example code cells to run \n",
    "\n",
    "Feel free to run as many of the below examples as you'd like, to get familiar with running code cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use python for arithmetic\n",
    "\n",
    "5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python len() function counts length of a string\n",
    "\n",
    "len(\"how many characters in this sentence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max function returns the largest item in an iterable \n",
    "\n",
    "list = [2, 75, 8, 98, 12, 97, 42, 3, 11]\n",
    "max(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Add New Code to Cells <a name=\"ex2\"></a>\n",
    "\n",
    "For the Python exercises in this notebook, you'll have the chance to try out ideas by adding new code into cells.  \n",
    "\n",
    "This exercise walks through how to add code into a cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switch Code Cells between Edit Mode and Command Mode \n",
    "\n",
    "All cells in the notebook have two functional modes: \n",
    "* **Command mode:** used for navigating and moving cells = Blue cell border    \n",
    "* **Edit mode:** used for adding/editing content in cells = Green cell border\n",
    "\n",
    "To put a cell into Edit Mode or Command mode, click on the cell. \n",
    "\n",
    "Clicking once on a Code cell puts it in Edit mode. You can tell the cell is in Edit mode because the cell border turns green. \n",
    "\n",
    "From Edit mode, to switch the Code cell into Command mode (blue border), press the `Esc` key.\n",
    "\n",
    "To try this click on the example Code cell below. The border should turn green = Edit mode.  \n",
    "\n",
    "To switch the cell back to Command mode, press the `Esc` key. The cell border will switch back to Blue = Command mode. \n",
    "\n",
    "**NOTE:** If you are not able to see Blue and Green, look for a cursor in the cell. If a cell is in Edit mode, you will see your cursor blinking in the cell. If there is no cursor blinking in the cell, it is in Command mode. \n",
    "\n",
    "Try switching the code cell below back and forth between Edit mode (green) and Command mode (blue) a few times to get used to how this works. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more example code\n",
    "\n",
    "print(\"Toggle a code cell between Edit and Command modes using Esc and Enter keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type or paste code into an existing cell\n",
    "\n",
    "In the exercises in this notebook, there are empty code cells available to try out solutions. You can paste example code provided, or type your own. \n",
    "\n",
    "First identify that it's a code cell. Look for the `In [ ]:` to the left of the cell, to make sure it's a code cell. Then add code to the cell. \n",
    "\n",
    "To start, try out pasting this line of sample code into the empty code cell below and then running it. \n",
    "\n",
    "1. Copy the block of text below starting from the `##`, and ending with the `)`  \n",
    "2. Then click on one of the empty code cells below, make sure the cell turns green for Edit mode, and paste the copied block of code. \n",
    "3. Then press `CTRL` + `ENTER` to run the code you pasted. \n",
    "\n",
    "```\n",
    "## comment \n",
    "print(\"here's an example of new code.\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the empty code cells below for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell\n",
    "\n",
    "## comment \n",
    "print(\"here's an example of new code.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example code cell with completed exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of completed exercise\n",
    "\n",
    "## comment \n",
    "print(\"here's an example of new code.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  More Jupyter Notebook Exercises: 'command palette', shortcuts, creating new cells <a name=\"mjn\"></a>\n",
    "\n",
    "Learn more about notebook commands, keyboard shortcuts, and creating new cells for Markdown and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The 'command palette' has all notebook actions and shortcuts\n",
    "\n",
    "In the command palette, you can search for any command, and run that action directly from the palette, as well as seeing the shortcut for that action if available. \n",
    "\n",
    "1. Press ``P`` to show the command palette.\n",
    "    * With the palette open, search for 'edit', to find the shortcut for `enter 'edit' mode`. Click on this action from the list. This cell will then switch into 'edit' mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keyboard Shortcuts for Jupyter Notebook :\n",
    "\n",
    "* `P` : show 'command palette'\n",
    "* `esc` : command mode\n",
    "* `enter` : edit mode\n",
    "* `a` : insert cell above\n",
    "* `b` : insert cell below\n",
    "*  `SHIFT` + `enter`: run a code cell (or render a markdown cell)\n",
    "* `d d`: delete a cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edit the text in a Markdown cell, execute (render) a Markdown cell\n",
    "\n",
    "* Switch this cell into 'edit mode' (green cell border), by double-clicking inside the cell. \n",
    "* Add a bullet point below these lines of text and type something, for example: ``'DONE'``. (You can copy/paste example below if easier!)\n",
    "* Then use `CTRL` + `Enter` to \"run\" the cell/render the markdown.  \n",
    "    * for example: `DONE`   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add new cells to notebook\n",
    "\n",
    "* With this cell in command mode (blue border), add a cell underneath it, then add some cells above the cells you added. \n",
    "    * press `b` to add a cell below an existing cell. \n",
    "    * You can keep pressing `b` to add more cells; it doesn't hurt anything to have empty cells in the notebook. \n",
    "    * press `a` to add a cell above an existing cell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new Markdown cell, add some text\n",
    "\n",
    "create a Markdown cell: \n",
    "* click on one of the empty cells you created, or just create another one here. \n",
    "* A brand-new cell will be in command mode (blue cell border)  \n",
    "* if there's an `In [ ]:` to the left of the cell, this means it's a code cell. Any text you type into the cell will be treated as code. \n",
    "* To convert the cell to a Markdown cell, press `m` to switch the cell to markdown.  \n",
    "    * The `In [ ]:` to the left of the cell will disappear, indicating it is now a Markdown cell. Any test you type into the cell will now be treated as markdown. \n",
    "* If this did not work, make sure the cell is in command mode. Switch to Command mode by pressing `Esc`. The cell border will turn blue. \n",
    "\n",
    "add text in a Markdown cell: \n",
    "* Switch the cell to Edit mode by clicking inside it. \n",
    "* In the markdown cell, type a header, and then some regular text. For example: \n",
    "\n",
    "    ```\n",
    "    #### here's an example header  \n",
    "    and some regular paragraph text \n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use the markdown cell below for the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###(placeholder markdown cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute/render a markdown cell\n",
    "\n",
    "* As you did for the code cells, press `CTRL` + `Enter` to \"run\" the markdown cell/render the markdown that you just typed.  \n",
    "* the text will display as a formatted version, and the cell border will switch from green to blue. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/pandas examples with collections metadata files  <a name=\"ppe\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Example 1: Explore datasets with pandas<a name=\"md1\"></a>\n",
    "\n",
    "This example walks through getting oriented with using python/pandas for viewing and analysing descriptive metadata files.\n",
    "\n",
    "In this scenario we are working with a group of separate metadata files that have varied sets of inconsistently organized fields. We want to assess completeness of specific fields across the group as a whole. \n",
    "\n",
    "We will import metadata from csv and tsv files, figure out which fields are relevant for our assessment, and select those from the raw datasheets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives in this example:\n",
    "\n",
    "* read a data file into a dataframe \n",
    "* create dataframes from differently delimited data\n",
    "* assess overall size and contents of the dataframe\n",
    "* view data in the dataframe\n",
    "* filter out irrelevant data\n",
    "* select relevant columns to include \n",
    "* identify basic datatype information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  Import libraries for Python\n",
    "\n",
    "Importing libraries loads them into memory so that Python can use them. \n",
    "\n",
    "Libraries provide specific methods for particular kinds of work. \n",
    "\n",
    "We are importing: \n",
    "* the pandas data analysis library : [pandas-docs Tutorials](https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html)\n",
    "* `os` for working with files and directories : [python docs os](https://docs.python.org/3/library/os.html)\n",
    "* `matplotlib` for generating some basic graphs from data : [matplotlib Users Guide](https://matplotlib.org/users/index.html)\n",
    "\n",
    "Setting `%matplotlib inline` allows plots to render within the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas and os libraries \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next: check that we are in the right place!\n",
    "\n",
    "We'll do a couple quick checks to get oriented and make sure that we're in the right directory. \n",
    "\n",
    "This is not really necessary, because this notebook is located in/running from within the 'notebook_exercises' subfolder, so we already know that we will be running commands relative to that location, but it's useful to know how to take a look around to see where you are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd() outputs the current working directory, similar to pwd in bash\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir() with no parameter returns a list of the files and directories in the current working directory\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use os.listdir() plus a parameter to see what's in the exampleData subfolder\n",
    "\n",
    "os.listdir('./exampleData/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe from one of the metadata sheets\n",
    "\n",
    "A dataframe is a Python object with rows and columns that can be selected for running calculations and manipulating the data in a lot of ways. \n",
    "\n",
    "You can read many different data formats into a dataframe, including csv, tsv, and even excel sheets. \n",
    "\n",
    "The command below uses the variable name 'maps' for creating a dataframe using the pandas `read_csv` function. \n",
    "\n",
    "   [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe named 'maps' from an example metadata file \n",
    "\n",
    "maps=pd.read_csv(\"./exampleData/maps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect a dataframe\n",
    "\n",
    "Next, we'll explore the maps dataframe with pandas attributes and methods, to get a sense of how large this datset is (how many rows and columns), what the column headers are and how many of them are empty, and what the datatypes are in each column. \n",
    "\n",
    "* shape [pandas.DataFrame.shape](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html)\n",
    "* columns [pandas.DataFrame.columns](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.columns.html)\n",
    "* info() [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.info.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape attribute displays the number of rows and columns for the dataframe, to get a sense of its overall size\n",
    "\n",
    "maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns attribute displays column labels\n",
    "\n",
    "maps.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the info method displays datatypes and numbers of values per column, and memory information\n",
    "\n",
    "maps.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datatypes in pandas\n",
    "\n",
    "The most common data types you will likely run into, and what they're used for:  \n",
    "\n",
    "* **object** - text (strings)\n",
    "* **int64** - positive (or negative whole numbers), with no decimal point \n",
    "* **float64** - decimal numbers \n",
    "* **datetime64** - date and time values\n",
    "\n",
    "For more information on datatypes:\n",
    "* [DataCarpentry.org tutorial on Data Types and Formats](https://datacarpentry.org/python-ecology-lesson/04-data-types-and-format/)\n",
    "* [Practical Business Python: Overview of Pandas Data Types](https://pbpython.com/pandas_dtypes.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting dataframe columns\n",
    "\n",
    "Since the columns listed above are not ordered in a logical way, it's hard to look for a particular column label in the output. \n",
    "\n",
    "However, dataframes can be sorted by rows, columns, or values. \n",
    "* [pandas.DataFrame.sort_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_index.html)\n",
    "* [pandas.DataFrame.sort_values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)\n",
    "\n",
    "Below we'll sort the dataframe to display the column labels alphabetically, so that it's easier to check if this collection has fields we're interested in, such as Title, Usage Rights, Date, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe to order the output of column labels from the info() method\n",
    "\n",
    "maps.sort_index(axis=1).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View contents of a dataframe \n",
    "\n",
    "The head() and tail() methods in pandas display the first or last n rows of data. \n",
    "* [pandas.DataFrame.head](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html)\n",
    "\n",
    "By default head or tails will display 5 rows; below we specify 3 to see fewer rows. \n",
    "\n",
    "Note that the column headers are no longer sorted alphabetically, because we did not apply the sort persistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore another dataset\n",
    "\n",
    "Now we'll create another dataframe from a different collection. \n",
    "\n",
    "Using shape and info() to get a basic sense of the data, we can see that this data sheet has nearly twice as many column labels, and tons of empty fields.  \n",
    "\n",
    "We filter the dataset by getting rid of the empty columns, and selecting a set of columns to focus on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify delimiter for read_csv\n",
    "\n",
    "You can also use the read_csv function for other kinds of delimiters, such as tab-delimited for this metadata file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in another example dataset as a separate dataframe. \n",
    "\n",
    "# use sep parameter to specify tab as delimiter\n",
    "\n",
    "rev=pd.read_csv(\"./exampleData/60001.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape to output number of rows and columns\n",
    "\n",
    "rev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe to order the output of column labels from the info() method\n",
    "\n",
    "rev.sort_index(axis=1).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get rid of empty columns\n",
    "\n",
    "Many of these columns are empty, so we will exclude them. \n",
    "\n",
    "The \"inplace\" attribute will apply this change to the dataframe we are currently working with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop empty columns. Inplace attribute overwrites the working dataframe. \n",
    "\n",
    "rev.dropna(axis = 1, how ='all', inplace = True)\n",
    "\n",
    "# output updated dataframe\n",
    "\n",
    "rev.sort_index(axis=1).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Methods to Series in a Dataframe\n",
    "\n",
    "Columns in the dataframe are a python object called a Series. You can use some methods at the level of Series as well as at the dataframe level. \n",
    "\n",
    "For example, the head() and tail() methods can also be applied to series as well as the whole dataframe. \n",
    "\n",
    "Below we use the head method to look at a list of values in the Title field in our dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first n rows of the Title column\n",
    "\n",
    "rev.Title.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chaining methods**\n",
    "\n",
    "You can chain methods together to apply another method to an object that has a method applied. \n",
    "\n",
    "Here we use the `sort_values` method to sort values in the Title series.  \n",
    "\n",
    "We use the 'ascending'=False parameter to sort reverse-alphabetically.  \n",
    "\n",
    "Then we apply the head() method to view the first 12 values of the sorted series. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first n values in the 'Title' series sorted alphabetically\n",
    "\n",
    "rev.Title.sort_values(ascending=False).head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Precisely Specify Locations in the Data\n",
    "\n",
    "The `.loc` method allows for selecting individual or multiple rows and columns.   \n",
    "* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)\n",
    "\n",
    "Below we'll focus on the ``'Title','Date', 'Usage Rights', and 'filename'`` columns. \n",
    "\n",
    "We use a `:` to include all rows of the dataframe. \n",
    "\n",
    "We will use the tail() method to view the last few rows from this selection.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify multiple rows and columns by label with loc method \n",
    "\n",
    "rev.loc[:,['Title','Date', 'Usage Rights', 'filename']].tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply the same columns to the maps dataframe**\n",
    "\n",
    "Meanwhile, our maps dataframe is still available in working memory. \n",
    "\n",
    "Because the same column labels exist in the maps dataset, we can select out these columns from maps using .loc, the same way as we did with the rev dataframe above.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the same columns in the 'maps' dataframe\n",
    "# tail method displays the last n rows\n",
    "\n",
    "maps.loc[:,['Title','Date', 'Usage Rights', 'filename']].tail(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Metadata Example 1\n",
    "\n",
    "In this example, we got familiar with our collections metadata, and identified fields to assess.  \n",
    "\n",
    "In the next example, we will select out the relevant fields from a group of files, compile separate dataframes into a single dataframe, and compare fields across collections. \n",
    "\n",
    "\n",
    "**Summary of example:** \n",
    "\n",
    "* reading CSV and tab-delimited metadata sheets into dataframes\n",
    "* exploring a dataframe as a whole, series within a dataframe, values in rows and columns\n",
    "* filtering datasets by removing empty and irrelevant columns\n",
    "* selecting columns to include\n",
    "* working with multiple dataframes at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Troubleshoot a Datatype Issue <a name=\"ex3\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the three collections in this example has Collection Number: 03883. The metadata filename is 03883.txt. \n",
    "\n",
    "What problems do you foresee for this collection number?  \n",
    "How would you check to see if the Collection Number values are correctly stored in the dataframe? \n",
    "\n",
    "Use the empty code cells below for this exercise: \n",
    "\n",
    "* Create a data frame from filename: './exampleData/03883.txt'\n",
    "* Using the types of methods we've used so far, check out the column names, values in each column, and datatypes assigned. \n",
    "* Take a look at the values in the Collection Number column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame from filename: './exampleData/03883.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shape attribute to summarize rows and columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) drop empty columns \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first n rows of the series: df['Collection Number']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell- use if needed \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Solution to Exercise 3: Troubleshoot a Datatype Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data frame from filename: './exampleData/03883.txt'\n",
    "\n",
    "df=pd.read_csv('./exampleData/03883.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shape attribute to summarize rows and columns.\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the info method to see column names and item counts in each column\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop empty columns \n",
    "\n",
    "df.dropna(axis = 1, how ='all', inplace = True)\n",
    "\n",
    "# again use the info method to see column names and item counts in each column\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view first n rows of a series \n",
    "\n",
    "df['Collection Number'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### specify datatype on import \n",
    "\n",
    "After identifying the problem, use the code below to re-create the dataframe, specifying the datatype for the problematic column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify datatype on import\n",
    "\n",
    "# re-create the dataframe, specifying datatype 'object' for 'Collection Number' column\n",
    "\n",
    "df=pd.read_csv('./exampleData/03883.txt',sep='\\t', dtype={'Collection Number':object})\n",
    "\n",
    "# again view first n rows of a series \n",
    "\n",
    "df['Collection Number'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell- use if needed \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Example 2: Compare a group of metadata files <a name=\"md2\"></a>\n",
    "\n",
    "In Example 1, we identified fields we want to analyze in our group of collections: Title, Date, Usage Rights, and a few Subject fields. \n",
    "\n",
    "In this example, we will select out the relevant fields from a group of datasheets by creating a variable to store the set of columns we're interested in.  \n",
    "\n",
    "Then we will concatenate metadata from the separate collections into a single dataframe that makes it easier to compare completeness of metadata across the group as a whole. \n",
    "\n",
    "We will generate basic graphs to show comparisons between the collections. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives in this example:   \n",
    "\n",
    "* Use a variable for storing a list\n",
    "* Select columns on import with `usecols` parameter\n",
    "* Create a new column\n",
    "* Use `.groupby` to organize data\n",
    "* Generate plots for visual comparisons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a variable to store the column names we want\n",
    "\n",
    "We create a variable 'collabels' to hold the list of the Column labels that we will then select out from each datasheet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable for column labels\n",
    "\n",
    "collabels=['Title','Date','Usage Rights', 'Subject Geographic', 'Subject Name' , 'Subject Topical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns and create a new column with Collection Names\n",
    "\n",
    "Now we can create a dataframe from each metadata sheet, including only the relevant Columns. \n",
    "\n",
    "We also create a new column in each dataframe that contains a simple collection name. We'll use these collection names when we group the metadata together into a single dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the same datasheets from the last example\n",
    "\n",
    "# create a new column in each dataframe that contain a short Collection Name value\n",
    "\n",
    "maps=pd.read_csv(\"./exampleData/maps.csv\", usecols=collabels)\n",
    "maps['colname'] ='Civil War Maps'\n",
    "\n",
    "rev=pd.read_csv(\"./exampleData/60001.txt\", usecols=collabels, sep='\\t')\n",
    "rev['colname'] ='Revolution Photographs'\n",
    "\n",
    "rmb=pd.read_csv(\"./exampleData/03883.txt\", sep='\\t', usecols=collabels)\n",
    "rmb['colname'] ='Roy M Brown Papers'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check our new dataframes \n",
    "\n",
    "Again use info() to see the value counts and datatypes in each Column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show info for updated dataframes \n",
    "\n",
    "maps.sort_index(axis=1).info()\n",
    "print('\\n')\n",
    "\n",
    "rev.sort_index(axis=1).info()\n",
    "print('\\n')\n",
    "\n",
    "rmb.sort_index(axis=1).info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate into single dataframe\n",
    "\n",
    "We create a new dataframe 'collstack', by concatenating the maps, rmb, and rev dataframes. \n",
    "\n",
    "Then we'll check that everything looks ok by looking at shape and info() for collstack. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes into a single dataframe \n",
    "\n",
    "collstack = pd.concat([maps, rmb, rev],ignore_index=True, axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collstack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collstack.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group the data by the Collection Names we created\n",
    "\n",
    "Now we can group the data by the Collection Names, and select fields to compare across the collections. \n",
    "\n",
    "The count() method counts the values in the Title and Usage Rights field for each collection. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by collection name, counting values in Title and Usage Rights columns\n",
    "\n",
    "collstack.groupby('colname')[['Title', 'Usage Rights']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Plots to Represent Comparisons Visually\n",
    "\n",
    "Next we'll generate some very basic plots to demonstrate working with pandas and matplotlib. \n",
    "\n",
    "Visualization is obviously a huge area in itself! These are not in any way impressive visualizations, but are just meant to give a little sense of pandas' flexible graphing capabilities. \n",
    "\n",
    "For example, by just adding `.plot(kind='bar')` to the code from the previous cell, we generate a bar graph of the data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bar graph for the Title/Usage Rights comparison\n",
    "\n",
    "collstack.groupby('colname')[['Title', 'Usage Rights']].count().plot(kind='bar', figsize=(8,6),width=0.8, )\n",
    "plt.ylabel('Count of items')\n",
    "plt.xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate Subjects fields across the collections \n",
    "\n",
    "We can include different column labels to focus on the subject fields instead, and plot the result. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collstack.groupby('colname')[['Title', 'Subject Geographic', 'Subject Name', 'Subject Topical']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a horizontal bar graph for the Subjects comparison\n",
    "\n",
    "collstack.groupby('colname')[['Title', 'Subject Geographic', 'Subject Name', 'Subject Topical']].count().plot(kind='barh', figsize=(8,6), width=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting with plot types and subsets of data\n",
    "\n",
    "By changing the value of the `kind=` parameter for `.plot`, we can experiment with different plot types to see which is best suited to a comparison. \n",
    "\n",
    "We can also use pandas methods to select a particular range of data to focus on in a plot. \n",
    "\n",
    "In this example, we focus on the people represented in the maps collection. We can sort the value counts in ascending order to produce a legible curve. \n",
    "\n",
    "If we want to focus on a smaller set within the group, we can use head() to focus on the most-represented names.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a horizontal bar graph for the Subjects Name counts in maps collection \n",
    "\n",
    "maps.groupby(['Subject Name']).Title.count().sort_values(ascending=False).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the barh graph to top 5 values\n",
    "\n",
    "maps.groupby(['Subject Name']).Title.count().sort_values(ascending=False).head().plot(kind='barh', figsize=(8,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Represent the same data as a pie plot\n",
    "\n",
    "Again changing the `kind=` parameter for `.plot` generates a different view of the same selection from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pie plot for the Subjects Name counts in maps collection \n",
    "\n",
    "maps.groupby(['Subject Name']).Title.count().sort_values(ascending=False).head().plot(kind='pie', figsize=(8,8),startangle=180)\n",
    "plt.ylabel('')\n",
    "#plt.title(\"Most Common Subject Names in Maps Collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other ways to reframe a selection\n",
    "\n",
    "Unsurprisingly, Jeremy Francis Gilmer is the most common name in the Gilmer maps collection. \n",
    "\n",
    "If we wanted to create a plot that excludes this obvious result, we could create a variable to select value counts that focus on a given range that we're more interested in. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group maps by Subject Name, counting Titles, view top n\n",
    "\n",
    "maps.groupby(['Subject Name']).Title.count().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable that specifies a range to examine\n",
    "# exclude the most-common heading to focus on the next seven values\n",
    "# create a pie plot from result\n",
    "\n",
    "subcounts=maps['Subject Name'].value_counts()\n",
    "\n",
    "totals=subcounts[(subcounts <= 103) & (subcounts>2)]\n",
    "\n",
    "totals.sort_values(ascending=False).plot(kind='pie', figsize=(8,8),startangle=90)\n",
    "plt.ylabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Metadata Example 2 \n",
    "\n",
    "In this example, we selected relevant fields from across separate datasheets and combined the data for comparison, using simple plots to assess collections visually. \n",
    "\n",
    "**summary of this example:**\n",
    "\n",
    "* use a variable to select a set of columns from multiple datasets \n",
    "* create a new column and add data\n",
    "* concatenate separate data into a single dataframe\n",
    "* organize datasets for comparison with groupby \n",
    "* compare datasets visually using basic graphs\n",
    "* explore different graph formats according to data attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluate values in a subject field in the maps collection<a name=\"ex4\"></a>\n",
    "\n",
    "\n",
    "In this exercise, we will take a closer look at values in the 'Subject Topical' field in the Maps collection. \n",
    "\n",
    "Based on the output below from maps.info(), it looks like the Subject Topical field is fairly complete in this collection. \n",
    "\n",
    "```\n",
    "maps.info()\n",
    "```\n",
    "\n",
    "```\n",
    "RangeIndex: 161 entries, 0 to 160\n",
    "Data columns (total 7 columns):\n",
    "Title                 161 non-null object\n",
    "Date                  161 non-null object\n",
    "Subject Topical       161 non-null object\n",
    "Subject Name          161 non-null object\n",
    "Subject Geographic    161 non-null object\n",
    "Usage Rights          0 non-null float64\n",
    "colname               161 non-null object\n",
    "```\n",
    "Try using the describe() method to get more details about the values represented within the 'Subject Topical' field. \n",
    "\n",
    "Would plotting the maps titles according to 'Subject Topical' fields assigned to them make an interesting graph? \n",
    "\n",
    "**To answer this question, add and run code into the empty code cells below to:**  \n",
    "\n",
    "* check the output from describe() for the maps dataframe\n",
    "* and to generate a plot for numbers of maps titles grouped by Subject Topical. \n",
    "\n",
    "---\n",
    "\n",
    "Here is code that you can paste into the empty code cells, (or feel free to type in the code!)\n",
    "\n",
    "1. check the output from describe() for the maps dataframe\n",
    "\n",
    "```\n",
    "\n",
    "# use describe() to evaluate the contents of the 'Subject Topical' field\n",
    "maps['Subject Topical'].describe()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "2. generate a plot for numbers of maps titles grouped by Subject Topical. \n",
    "\n",
    "```\n",
    "\n",
    "# generate a plot for the distribution of Subject Topical field across titles\n",
    "maps.groupby(['Subject Topical']).Title.count().plot(kind='barh',color=['grey'])\n",
    "plt.xlabel('item count')\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "For reference, the solution is also demonstrated in the next set of cells below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use these empty code cells for the exercise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution to Exercise 4: Evaluate Subjects data in a collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from `describe()` shows that although every item in this collection has a Subject Topical field, it is all the same value. \n",
    "\n",
    "So, plotting the Title counts for the Subject Topical field does not make a very interesting plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use describe() to evaluate the contents of the 'Subject Topical' field\n",
    "\n",
    "maps['Subject Topical'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a plot for the distribution of Subject Topical field across titles\n",
    "\n",
    "maps.groupby(['Subject Topical']).Title.count().plot(kind='barh',color=['grey'])\n",
    "plt.xlabel('item count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell - use if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Example 3: Merge information from separate files <a name=\"md3\"></a>\n",
    "\n",
    "Another useful feature of pandas is that it allows you to do SQL-like joins with datasheets.  \n",
    "\n",
    "In this exercise, we will create a merged dataframe from descriptive metadata and file sizes information in separate datasets. \n",
    "\n",
    "We will rename columns in the descriptive metadata dataframe to merge based on columns in our filesizes datasheets.  \n",
    "(It's also possible to specify the columns to merge separately for the left and right dataframes if they are not named the same!)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives in this example:  \n",
    "\n",
    "* Review of removing empty columns\n",
    "* Review of datatypes and potential errors, specifying datatypes for columns\n",
    "* Rename columns [pandas.DataFrame.rename](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html)\n",
    "* Merge dataframes [pandas.DataFrame.merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)\n",
    "* Write a dataframe to a csv output file or other format \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the import statement below and run this cell if your notebook was reset and you need the libraries again\n",
    "\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of collections metadata in this example\n",
    "\n",
    "In this example, we have three separate datasheets about various files from a single collection. We are going to combine them into one dataframe by merging on filenames. \n",
    "\n",
    "* descriptive metadata about the items\n",
    "* technical metadata about the access files\n",
    "* technical metadata about the 'master' files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe for descriptive metadata\n",
    "\n",
    "As in previous examples, we will create a dataframe and check out its overall size, columns, and value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in collections metadata file as 'metadata' dataframe\n",
    "metadata=pd.read_csv(\"./exampleData/03823_metadata.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use shape attribute to summarize rows and columns.\n",
    "\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the info method to see column names and item counts in each column\n",
    "\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix datatype for collection number and drop empty columns\n",
    "\n",
    "This collection has a ton of empty fields, so we will again drop unused columns.  \n",
    "\n",
    "Also note that as in the exercise above, 'Collection Number' is getting cast as an `int64` datatype. \n",
    "\n",
    "We will fix this by re-creating the dataframe, specifying `object` datatype for this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect first n values of Collection Number column \n",
    "\n",
    "metadata['Collection Number'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create data frame specifying 'object' datatype for Collection Number column\n",
    "\n",
    "# take another look at first n values of Colection Number column\n",
    "\n",
    "metadata=pd.read_csv(\"./exampleData/03823_metadata.txt\", sep='\\t', dtype={'Collection Number':object})\n",
    "\n",
    "metadata['Collection Number'].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the empty columns using dropna\n",
    "\n",
    "# and re-check the column names and item counts by re-running the info method on the reshaped dataset.\n",
    "\n",
    "metadata.dropna(axis = 1, how ='all', inplace = True)\n",
    "\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify which field to merge with Access Files\n",
    "\n",
    "We will take a look at a few rows to see which field has the filenames we are matching to access files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the head method to see the first n rows\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create access files dataframe \n",
    "\n",
    "Now we'll create a dataframe from our access files datasheet, checking information about the column labels and values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the second dataframe with the access files datasheet\n",
    "\n",
    "accfiles=pd.read_csv(\"./exampleData/03823_access_images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the info method to see column names and item counts in each column\n",
    "\n",
    "accfiles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns and rename them for clarity\n",
    "\n",
    "We only need Name, Full Path, and file size information from this sheet, so we will re-create our dataframe using only these fields. \n",
    "\n",
    "We will also rename fields to identify data about access files in the merged sheet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the second dataframe with the access files datasheet\n",
    "\n",
    "accfiles=pd.read_csv(\"./exampleData/03823_access_images.csv\", usecols=['Name','Full Path', 'Size'])\n",
    "\n",
    "accfiles.rename(columns={'Name':'AccessName', 'Full Path':'AccessFilePath', 'Size' : 'AccessFileSize'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-check acccess files sheet \n",
    "\n",
    "The dataframe now has only relevant columns, labelled so that they are identifiable as access files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shape and info to take a look at the accfiles dataframe.\n",
    "\n",
    "print(accfiles.shape) \n",
    "\n",
    "accfiles.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View data in the field to merge  \n",
    "\n",
    "The AccessName field has the filenames that will be merged with the descriptive metadata. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the head method to see the first n rows of a column\n",
    "\n",
    "accfiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename column and merge files  \n",
    "\n",
    "To simpify the merge, we will rename the field in the metadata sheet to \"AccessName\". \n",
    "\n",
    "Then we perform a left merge, matching access files to filenames in the metadata dataframe.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename a column in 'metadata' dataframe\n",
    "\n",
    "metadata.rename(columns={'Object file name':'AccessName'}, inplace=True)\n",
    "metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join access files data onto the metadata dataframe\n",
    "\n",
    "combined = pd.merge(metadata, accfiles,on='AccessName', how='left')\n",
    "\n",
    "# and view info for the merged dataframe\n",
    "\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use string replace to clean up file sizes \n",
    "\n",
    "Notice in the info() output above that 'AccessFileSize' column has datatype 'object'. \n",
    "\n",
    "This is because the units are included in the values for this column!    \n",
    "\n",
    "We can use Python's string replace method to get rid of these, and create a new column, 'AccessSizeNum', with datatype of integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n values of column with head()\n",
    "\n",
    "combined.AccessFileSize.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new AccessSizeNum column from AccessFileSize values with ' Bytes' removed\n",
    "\n",
    "combined['AccessSizeNum'] = combined.AccessFileSize.apply(lambda x: x.replace(' Bytes',''))\n",
    "\n",
    "# convert new AccessSizeNum column to datatype 'int64', and check values in dataframe\n",
    "\n",
    "combined=combined.astype({'AccessSizeNum':'int64'})\n",
    "\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe from masters file data\n",
    "\n",
    "Now we can apply everything we've done with the access files sheet to the masters file sheet. \n",
    "\n",
    "We create a dataframe with relevant columns that we rename as master files-specific, and create a numeric column with file sizes data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a third dataframe with the masters files datasheet\n",
    "\n",
    "masters=pd.read_csv(\"./exampleData/03823_masters.csv\", usecols=['Name','Full Path', 'Size'])\n",
    "\n",
    "masters.rename(columns={'Name':'MastersName', 'Full Path':'MasterFilePath', 'Size' : 'MasterFileSize'}, inplace=True)\n",
    "\n",
    "masters['MasterSizeNum'] = masters.MasterFileSize.apply(lambda x: x.replace(' Bytes',''))\n",
    "\n",
    "masters=masters.astype({'MasterSizeNum':'int64'})\n",
    "\n",
    "masters.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View data in the fields to merge  \n",
    "\n",
    "The MastersName field has the filenames that will be merged with the descriptive metadata, which is in the 'filename' column in that sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masters.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the masterfiles info with the combined dataframe\n",
    "\n",
    "As above, we'll rename the column we want to merge to. \n",
    "\n",
    "Then we create a new dataframe that contains data selected from all three of the original files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.rename(columns={'filename':'MastersName'}, inplace=True)\n",
    "\n",
    "combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all3 = pd.merge(combined, masters,on='MastersName', how='left')\n",
    "\n",
    "all3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all3.loc[:,['Collection Number','Object', 'MastersName', 'AccessName', 'AccessSizeNum','MasterSizeNum']].head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output to a csv file\n",
    "\n",
    "We can write the dataframe to a CSV file, saved to the output folder of this repository. \n",
    "\n",
    "The code below will include all columns. You can also uncomment and run the other line of code in this cell to generate a CSV from a selection of columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataframe to output file \n",
    "\n",
    "all3.to_csv('./output/all3.csv', index=False,encoding='utf-8')\n",
    "\n",
    "# uncomment the line below to select specific columns to write to csv\n",
    "\n",
    "# all3.loc[:,['Collection Number','Object', 'MastersName', 'AccessName', 'AccessSizeNum','MasterSizeNum']].to_csv('./output/all3_selectcolumns.csv', index=False,encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Metadata Example 3\n",
    "\n",
    "In this example, we selected and merged data from separate sheets into a single dataframe, from which we selected and output to a CSV file.   \n",
    "\n",
    "**Summary of example:** \n",
    "\n",
    "* review of selecting and renaming columns, converting column datatype\n",
    "* create new dataframe by merging on a column\n",
    "* use string replace in a series \n",
    "* write to csv output file \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exercise 5: Other ways to merge <a name=\"ex5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate access files data, merge to keep extra rows\n",
    "\n",
    "The access files sheet contained more rows than the descriptive metadata file we merged onto. Because we used a left merge, any rows that did not contain an \"AccessName\" matching the descriptive metadata sheet we were merging to were excluded.\n",
    "\n",
    "In this exercise, we will investigate these extra rows and try different options to include them in our merged dataframe. \n",
    "\n",
    "Use methods to answer the following questions: \n",
    "\n",
    "* What are these extra files?\n",
    "* How many more files are there in the accfiles than the metadata dataframe? \n",
    "* How could you change the merge to include all files from accfiles? \n",
    "\n",
    "For creating a new merge, use a new variable name that reflects the merge performed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell - use for exercise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell - use for exercise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell - use for exercise \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty code cell - use for exercise \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Solution to Exercise 5: Other ways to merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tail method to see the last n rows of a column\n",
    "accfiles.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use shape to check the number of rows in the metadata vs. accfiles dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output row and column count for metadata and accfiles dataframes \n",
    "\n",
    "metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output row and column count for metadata and accfiles dataframes \n",
    "\n",
    "accfiles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For checking the number of additional files, you might try string count method:   \n",
    "\n",
    "[pandas.Series.str.count](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.count.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use python string method to count instances of 'icon'\n",
    "\n",
    "accfiles['AccessName'].str.count(\"icon\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to include the extra rows from accfiles in a merged dataframe, there are a few options. \n",
    "\n",
    "The simplest would be to use a left merge but reverse the order. This is demonstrated below - note that left merge preserves the order of the keys from the left frame, so the key order is different when left and right are reversed. \n",
    "\n",
    "Another option is to use a different merge type. An outer merge uses the union of keys from both frames. Try an outer merge as well and compare the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse order of left merge - adds metadata dataframe to access files data\n",
    "\n",
    "accleft = pd.merge(accfiles, metadata, on='AccessName', how='left')\n",
    "\n",
    "# and view info for the merged dataframe\n",
    "\n",
    "accleft.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use outer merge \n",
    "\n",
    "outercombine = pd.merge(metadata, accfiles, on='AccessName', how='outer')\n",
    "\n",
    "outercombine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Merge indicator to track source\n",
    "\n",
    "If using an outer merge, you can set parameter `indicator=True` to show which source data contained the merge field value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use outer merge with merge indicator\n",
    "\n",
    "outercombine = pd.merge(metadata, accfiles, on='AccessName', how='outer', indicator=True)\n",
    "\n",
    "outercombine.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outercombine.loc[:,['AccessName', '_merge']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Example: Datatypes and Working with Dates<a name=\"md4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 'to_datetime' to convert datatype in Dates column\n",
    "\n",
    "In this example, we will go back to our rev dataframe and examine the datatype for the Dates field.  \n",
    "\n",
    "Notice the format of the values in the 'Date' column. We can also see in the info() output that the datatype for 'Date' is currently 'object'. Right now python is viewing these dates as just strings, which limits what we can do with them. \n",
    "\n",
    "We can create a new column in the dataframe, using the `to_datetime` method to convert the 'Date' values to datatype 'datetime', which then has a lot of capabilities available to it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check first n values in Date series\n",
    "\n",
    "rev.Date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes in rev dataframe \n",
    "\n",
    "rev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current datatype and values information for the `Date` column in the rev dataframe\n",
    "\n",
    "rev.Date.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'datesformat', with the values from the Date column \n",
    "# Use to_datetime to convert the 'Date' values to the datetime datatype\n",
    "\n",
    "rev['datesformat']=pd.to_datetime(rev['Date'])\n",
    "\n",
    "rev.datesformat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current datatype and values information for the new `datesformat` column in the rev dataframe\n",
    "\n",
    "rev.datesformat.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the dataframe with new column added\n",
    "\n",
    "Note that the new column will be added at the end of the dataframe. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the columns and dtypes in the rev dataframe\n",
    "\n",
    "rev.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources <a name=\"res\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Python and data science libraries\n",
    "\n",
    "Anaconda - package and environment manager for Python\n",
    "https://docs.anaconda.com/anaconda/install/\n",
    "\n",
    "Datacamp tutorials on Anaconda \n",
    "* [Installing Anaconda on Windows](https://www.datacamp.com/community/tutorials/installing-anaconda-windows) \n",
    "* [Intsalling Anaconda on Mac OS X](https://www.datacamp.com/community/tutorials/installing-anaconda-mac-os-x)\n",
    "\n",
    "Python.org Guide: [Installing packages using pip and virtual environments](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/)\n",
    "* how to manually create/manage Python environments if not using Anaconda "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started with pandas: documentation and tutorials \n",
    "\n",
    "Pandas official documentation - User Guide   \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html   \n",
    "\n",
    "Dataquest pandas tutorial   \n",
    "https://www.dataquest.io/blog/pandas-python-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataschool.io - excellent open education site with tons of linked resources\n",
    "Python pandas Q&A video series - (tutorials on you tube) \n",
    "https://github.com/justmarkham/pandas-videos\n",
    "\n",
    "pandas tips and tricks (daily series)\n",
    "https://www.dataschool.io/python-pandas-tips-and-tricks/\n",
    "\n",
    "List of best pandas resources \n",
    "https://www.dataschool.io/best-python-pandas-resources/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific topics and tutorials \n",
    "read_csv tutorial   \n",
    "https://www.datacamp.com/community/tutorials/pandas-read-csv\n",
    "\n",
    "Data cleaning tutorial   \n",
    "https://www.dataquest.io/blog/data-cleaning-with-python/\n",
    "\n",
    "Using pandas with text data   \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html\n",
    "\n",
    "Pandas pivot table tutorial   \n",
    "https://pbpython.com/pandas-pivot-table-explained.html\n",
    "\n",
    "Data Types and Formats   \n",
    "https://datacarpentry.org/python-ecology-lesson/04-data-types-and-format/\n",
    "\n",
    "Jupyter Notebook about pandas to SQL    \n",
    "https://nbviewer.jupyter.org/github/gjreda/pydata2014nyc/blob/master/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python/pandas projects in a libraries/archives context\n",
    "\n",
    "\"Analyzing EZproxy SPU Logs Using Python Data Analysis Tools\"   \n",
    "https://journal.code4lib.org/articles/13918\n",
    "\n",
    "\"Leveraging Python to improve ebook metadata selection, ingest, and management\"   \n",
    "https://journal.code4lib.org/articles/12828\n",
    "\n",
    "Metadata Analytics, Visualization, and Optimization: Experiments in statistical analysis of the Digital Public Library of America (DPLA)   \n",
    "https://journal.code4lib.org/articles/11752\n",
    "\n",
    "\"Python, Google Sheets, and the Thesaurus for Graphic Materials for Efficient Metadata Project Workflows\"   \n",
    "https://journal.code4lib.org/articles/12182\n",
    "* UVa project scripts: (https://github.com/jtbmadva/bicentennial-project-scripts)\n",
    "\n",
    "\"Of Python and Pandas: Using Programming to Improve Discovery and Access\"   \n",
    "https://saaers.wordpress.com/2018/10/09/of-python-and-pandas-using-programming-to-improve-discovery-and-access/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
